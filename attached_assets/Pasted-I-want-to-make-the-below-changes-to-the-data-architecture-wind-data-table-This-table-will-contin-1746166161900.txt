I want to make the below changes to the data architecture 

wind_data table 

This table will continue receiving all valid incoming readings from the UDP listener service. It will act as a short-term buffer, retaining only the last 180 minutes of data, and will support:
The real-time UI (providing the latest reading per device)
Aggregation into a new historical data table
wind_data_historical table (New)
This new table will store 10-minute aggregated data for each device, including:
Average wind speed
Maximum wind speed
Standard deviation
Alert statuses triggered
Cumulative downtime - you will need to build API endpoint for downtime reporting as this doesn't exist yet
The wind_data_historical table will serve all statistical endpoints now except for current wind speed, which will still be provided by the wind_data table.

Data Flow Summary
On receiving data via UDP, insert the reading into wind_data (as is currently done). Consider adding a DB index to optimize real-time UI queries for the "latest" wind speed per device.

A new aggregation job will run every 10 minutes (e.g., at :10, :20, :30...) to:

Query the last 10 minutes of data from wind_data
Aggregate metrics by device
Insert results into wind_data_historical
Consider adding a "processed" flag to track which readings have been aggregated
For aggregations, consider window functions if appropriate in your queries for time-based analysis
This job must:

Track the last processed interval to avoid duplicate aggregation
Include retry or recovery logic in case of failures
We also need to periodically delete rows in wind_data older than 180 minutes to maintain the short-term buffer.

Things to watch out for: 

Race conditions between the new aggregation job and UDP inserts
Ensuring no data loss during the aggregation process
The new wind_data_historical table will need careful schema design to support efficient querying of historical trends
The udp-listener.ts receives timestamps from devices but doesn't explicitly handle timezone information. If devices send data with different timezone offsets, this could cause aggregation windows to be misaligned.
Timezone / timestamp issues - to mitigate:
- Store all timestamps in UTC in the database
- Define aggregation windows in UTC
- Convert to local time only at display time
- Include timezone information in device metadata
- Add timezone handling to the aggregation job logic